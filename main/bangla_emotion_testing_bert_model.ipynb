{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f9c5a0",
   "metadata": {},
   "source": [
    "# import #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719d0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d01938",
   "metadata": {},
   "source": [
    "# fnc #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1877a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "def train_test_debug():\n",
    "    if train_set[\"text\"][0] == training_data1[\"text\"][0]:\n",
    "        print(\"train set is ***1***\")\n",
    "    \n",
    "    if train_set[\"text\"][0] == training_data2[\"text\"][0]:\n",
    "        print(\"train set is ***2***\")\n",
    "        \n",
    "    if test_set[\"text\"][0] == testing_data1[\"text\"][0]:\n",
    "        print(\"test set is ***1***\")\n",
    "    \n",
    "    if test_set[\"text\"][0] == testing_data2[\"text\"][0]:\n",
    "        print(\"test set is ***2***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcdf49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = str(text)\n",
    "    if \",\" in text:\n",
    "        return str(text.replace(\",\",\"\"))\n",
    "    if \".\" in text:\n",
    "        return str(text.replace(\".\",\"\"))\n",
    "    if \"ред\" in text:\n",
    "        return str(text.replace(\"ред\",\"\"))\n",
    "    if \"|\" in text:\n",
    "        return str(text.replace(\"|\",\"\"))\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae497abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion(emotion):\n",
    "    if emotion == \"happy\":\n",
    "        return 0\n",
    "    elif emotion == \"sad\":\n",
    "        return 1\n",
    "    elif emotion == \"angry\":\n",
    "        return 2\n",
    "    elif emotion == \"disgust\":\n",
    "        return 3\n",
    "    elif emotion == \"fear\":\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afe232",
   "metadata": {},
   "source": [
    "# data #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab50943",
   "metadata": {},
   "source": [
    "**dataset1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2fb20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reaction\n",
      "angry      [text, reaction]\n",
      "disgust    [text, reaction]\n",
      "fear       [text, reaction]\n",
      "happy      [text, reaction]\n",
      "sad        [text, reaction]\n",
      "dtype: object\n",
      "shape = (3096, 2)\n",
      "dtype = text        object\n",
      "reaction    object\n",
      "dtype: object\n",
      "value count = \n",
      "happy      1086\n",
      "sad         721\n",
      "angry       720\n",
      "disgust     357\n",
      "fear        212\n",
      "Name: reaction, dtype: int64\n",
      "\n",
      "\n",
      "reaction\n",
      "angry      [text, reaction]\n",
      "disgust    [text, reaction]\n",
      "fear       [text, reaction]\n",
      "happy      [text, reaction]\n",
      "sad        [text, reaction]\n",
      "dtype: object\n",
      "shape = (1032, 2)\n",
      "dtype = text        object\n",
      "reaction    object\n",
      "dtype: object\n",
      "value count = \n",
      "happy      351\n",
      "angry      269\n",
      "sad        224\n",
      "disgust    112\n",
      "fear        76\n",
      "Name: reaction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "training_data1 = pd.read_excel(\"all_data/training_DS1.xlsx\")\n",
    "print(training_data1.groupby(\"reaction\").apply(list))\n",
    "print(f\"shape = {training_data1.shape}\")\n",
    "print(f\"dtype = {training_data1.dtypes}\")\n",
    "print(f\"value count = \\n{training_data1['reaction'].value_counts()}\")\n",
    "\n",
    "training_list1 = training_data1.values.tolist()\n",
    "print(\"\\n\")\n",
    "testing_data1 = pd.read_excel(\"all_data/testing_DS1.xlsx\")\n",
    "print(testing_data1.groupby(\"reaction\").apply(list))\n",
    "print(f\"shape = {testing_data1.shape}\")\n",
    "print(f\"dtype = {testing_data1.dtypes}\")\n",
    "print(f\"value count = \\n{testing_data1['reaction'].value_counts()}\")\n",
    "\n",
    "testing_list1 = training_data1.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7b0297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " split \n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data1[\"text\"] = training_data1[\"text\"].apply(clean_text)\n",
    "training_data1[\"label\"] = training_data1[\"reaction\"].apply(get_emotion)\n",
    "training_data1[\"num_of_words\"] = training_data1[\"text\"].apply(lambda x:len(str(x).split()))\n",
    "\n",
    "print(\"\\n split \\n\")\n",
    "\n",
    "testing_data1[\"text\"] = testing_data1[\"text\"].apply(clean_text)\n",
    "testing_data1[\"label\"] = testing_data1[\"reaction\"].apply(get_emotion)\n",
    "testing_data1[\"num_of_words\"] = testing_data1[\"text\"].apply(lambda x:len(str(x).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4840f",
   "metadata": {},
   "source": [
    "**dataset2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19372d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reaction\n",
      "angry      [text, reaction]\n",
      "disgust    [text, reaction]\n",
      "fear       [text, reaction]\n",
      "happy      [text, reaction]\n",
      "sad        [text, reaction]\n",
      "dtype: object\n",
      "shape = (3720, 2)\n",
      "dtype = text        object\n",
      "reaction    object\n",
      "dtype: object\n",
      "value count = \n",
      "happy      883\n",
      "sad        810\n",
      "disgust    799\n",
      "angry      629\n",
      "fear       599\n",
      "Name: reaction, dtype: int64\n",
      "\n",
      "\n",
      "reaction\n",
      "angry      [text, reaction]\n",
      "disgust    [text, reaction]\n",
      "fear       [text, reaction]\n",
      "happy      [text, reaction]\n",
      "sad        [text, reaction]\n",
      "dtype: object\n",
      "shape = (1265, 2)\n",
      "dtype = text        object\n",
      "reaction    object\n",
      "dtype: object\n",
      "value count = \n",
      "happy      311\n",
      "disgust    289\n",
      "sad        275\n",
      "angry      208\n",
      "fear       182\n",
      "Name: reaction, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "training_data2 = pd.read_excel(\"all_data/training_DS2.xlsx\")\n",
    "print(training_data2.groupby(\"reaction\").apply(list))\n",
    "print(f\"shape = {training_data2.shape}\")\n",
    "print(f\"dtype = {training_data2.dtypes}\")\n",
    "print(f\"value count = \\n{training_data2['reaction'].value_counts()}\")\n",
    "\n",
    "training_list2 = training_data2.values.tolist()\n",
    "print(\"\\n\")\n",
    "testing_data2 = pd.read_excel(\"all_data/testing_DS2.xlsx\")\n",
    "print(testing_data2.groupby(\"reaction\").apply(list))\n",
    "print(f\"shape = {testing_data2.shape}\")\n",
    "print(f\"dtype = {testing_data2.dtypes}\")\n",
    "print(f\"value count = \\n{testing_data2['reaction'].value_counts()}\")\n",
    "\n",
    "testing_list2 = training_data2.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49a106e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reaction\n",
      "angry      [text, reaction, label, num_of_words]\n",
      "disgust    [text, reaction, label, num_of_words]\n",
      "fear       [text, reaction, label, num_of_words]\n",
      "happy      [text, reaction, label, num_of_words]\n",
      "sad        [text, reaction, label, num_of_words]\n",
      "dtype: object\n",
      "\n",
      " split \n",
      "\n",
      "reaction\n",
      "angry      [text, reaction, label, num_of_words]\n",
      "disgust    [text, reaction, label, num_of_words]\n",
      "fear       [text, reaction, label, num_of_words]\n",
      "happy      [text, reaction, label, num_of_words]\n",
      "sad        [text, reaction, label, num_of_words]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "training_data2[\"text\"] = training_data2[\"text\"].apply(clean_text)\n",
    "training_data2[\"label\"] = training_data2[\"reaction\"].apply(get_emotion)\n",
    "training_data2[\"num_of_words\"] = training_data2[\"text\"].apply(lambda x:len(str(x).split()))\n",
    "print(training_data2.groupby(\"reaction\").apply(list))\n",
    "\n",
    "print(\"\\n split \\n\")\n",
    "\n",
    "testing_data2[\"text\"] = testing_data2[\"text\"].apply(clean_text)\n",
    "testing_data2[\"label\"] = testing_data2[\"reaction\"].apply(get_emotion)\n",
    "testing_data2[\"num_of_words\"] = testing_data2[\"text\"].apply(lambda x:len(str(x).split()))\n",
    "print(training_data2.groupby(\"reaction\").apply(list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd8d1b4",
   "metadata": {},
   "source": [
    "**select dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31daaf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = None\n",
    "test_set = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379a619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_train_test(train,test):\n",
    "    global train_set\n",
    "    global test_set\n",
    "    if train == 1 and test == 1:\n",
    "        print(\"train = 1 test = 1 loaded\")\n",
    "        print(testing_data1.groupby(\"reaction\").apply(list))\n",
    "        train_set = training_data1.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "        test_set = testing_data1.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "    elif train == 2 and test == 2:\n",
    "        print(\"train = 2 test = 2 loaded\")\n",
    "        print(training_data2.groupby(\"reaction\").apply(list))\n",
    "        train_set = training_data2.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "        test_set = testing_data2.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "    elif train == 1 and test == 2:\n",
    "        print(\"train = 1 test = 2 loaded\")\n",
    "        train_set = training_data1.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "        test_set = testing_data2.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "    elif train == 2 and test == 1:\n",
    "        print(\"train = 2 test = 1 loaded\")\n",
    "        train_set = training_data2.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "        test_set = testing_data1.drop(columns=[\"reaction\",\"num_of_words\"])\n",
    "    else:\n",
    "        print(\"choose between train 1/2 and test 1/2\")\n",
    "        \n",
    "    print(f\"value count = \\n{train_set['label'].value_counts()}\")\n",
    "    print(f\"value count = \\n{test_set['label'].value_counts()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b302a952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d8232",
   "metadata": {},
   "source": [
    "# Select train test #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cf0519f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 1 test = 1 loaded\n",
      "reaction\n",
      "angry      [text, reaction, label, num_of_words]\n",
      "disgust    [text, reaction, label, num_of_words]\n",
      "fear       [text, reaction, label, num_of_words]\n",
      "happy      [text, reaction, label, num_of_words]\n",
      "sad        [text, reaction, label, num_of_words]\n",
      "dtype: object\n",
      "value count = \n",
      "0    1086\n",
      "1     721\n",
      "2     720\n",
      "3     357\n",
      "4     212\n",
      "Name: label, dtype: int64\n",
      "value count = \n",
      "0    351\n",
      "2    269\n",
      "1    224\n",
      "3    112\n",
      "4     76\n",
      "Name: label, dtype: int64\n",
      "train set is ***1***\n",
      "test set is ***1***\n"
     ]
    }
   ],
   "source": [
    "select_train_test(1,1)\n",
    "train_test_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26c0df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 2 test = 2 loaded\n",
      "reaction\n",
      "angry      [text, reaction, label, num_of_words]\n",
      "disgust    [text, reaction, label, num_of_words]\n",
      "fear       [text, reaction, label, num_of_words]\n",
      "happy      [text, reaction, label, num_of_words]\n",
      "sad        [text, reaction, label, num_of_words]\n",
      "dtype: object\n",
      "value count = \n",
      "0    883\n",
      "1    810\n",
      "3    799\n",
      "2    629\n",
      "4    599\n",
      "Name: label, dtype: int64\n",
      "value count = \n",
      "0    311\n",
      "3    289\n",
      "1    275\n",
      "2    208\n",
      "4    182\n",
      "Name: label, dtype: int64\n",
      "train set is ***2***\n",
      "test set is ***2***\n"
     ]
    }
   ],
   "source": [
    "select_train_test(2,2)\n",
    "train_test_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef2438dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 1 test = 2 loaded\n",
      "value count = \n",
      "0    1086\n",
      "1     721\n",
      "2     720\n",
      "3     357\n",
      "4     212\n",
      "Name: label, dtype: int64\n",
      "value count = \n",
      "0    311\n",
      "3    289\n",
      "1    275\n",
      "2    208\n",
      "4    182\n",
      "Name: label, dtype: int64\n",
      "train set is ***1***\n",
      "test set is ***2***\n"
     ]
    }
   ],
   "source": [
    "select_train_test(1,2)\n",
    "train_test_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "720114eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train = 2 test = 1 loaded\n",
      "value count = \n",
      "0    883\n",
      "1    810\n",
      "3    799\n",
      "2    629\n",
      "4    599\n",
      "Name: label, dtype: int64\n",
      "value count = \n",
      "0    351\n",
      "2    269\n",
      "1    224\n",
      "3    112\n",
      "4     76\n",
      "Name: label, dtype: int64\n",
      "train set is ***2***\n",
      "test set is ***1***\n"
     ]
    }
   ],
   "source": [
    "select_train_test(2,1)\n",
    "train_test_debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8a909b",
   "metadata": {},
   "source": [
    "# Model \"monsoon-nlp/bangla-electra\" #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b7be5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type electra to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at monsoon-nlp/bangla-electra were not used when initializing BertForSequenceClassification: ['electra.embeddings.LayerNorm.bias', 'electra.encoder.layer.11.attention.self.query.bias', 'electra.encoder.layer.4.attention.output.dense.bias', 'electra.encoder.layer.2.attention.output.dense.bias', 'electra.encoder.layer.6.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.self.value.weight', 'electra.encoder.layer.3.attention.self.key.weight', 'electra.encoder.layer.2.attention.self.query.bias', 'electra.encoder.layer.4.output.LayerNorm.weight', 'electra.encoder.layer.8.attention.self.value.bias', 'electra.encoder.layer.10.attention.self.query.bias', 'electra.embeddings_project.weight', 'electra.encoder.layer.4.output.LayerNorm.bias', 'electra.encoder.layer.11.attention.self.value.bias', 'electra.encoder.layer.5.attention.output.dense.weight', 'electra.encoder.layer.7.attention.output.dense.bias', 'electra.encoder.layer.0.attention.self.key.weight', 'electra.encoder.layer.0.output.dense.bias', 'electra.encoder.layer.1.output.dense.bias', 'electra.encoder.layer.9.attention.self.value.bias', 'electra.encoder.layer.3.output.LayerNorm.bias', 'electra.encoder.layer.4.attention.self.value.weight', 'electra.encoder.layer.4.attention.output.dense.weight', 'electra.encoder.layer.1.attention.self.key.bias', 'electra.encoder.layer.5.output.dense.bias', 'electra.encoder.layer.1.intermediate.dense.bias', 'electra.encoder.layer.0.attention.output.LayerNorm.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.output.dense.weight', 'electra.encoder.layer.8.attention.output.dense.weight', 'electra.encoder.layer.6.attention.self.value.weight', 'electra.encoder.layer.3.attention.self.query.bias', 'electra.encoder.layer.11.attention.self.key.bias', 'electra.encoder.layer.1.attention.self.value.bias', 'electra.encoder.layer.0.attention.output.dense.weight', 'electra.encoder.layer.11.output.dense.weight', 'electra.encoder.layer.5.attention.self.query.bias', 'electra.encoder.layer.11.intermediate.dense.weight', 'electra.encoder.layer.5.attention.self.value.bias', 'electra.encoder.layer.11.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.output.LayerNorm.weight', 'electra.encoder.layer.0.intermediate.dense.weight', 'electra.encoder.layer.10.attention.output.dense.weight', 'electra.encoder.layer.3.attention.self.value.bias', 'electra.encoder.layer.9.attention.self.query.bias', 'electra.encoder.layer.7.attention.output.LayerNorm.weight', 'electra.encoder.layer.5.output.LayerNorm.weight', 'electra.encoder.layer.7.output.LayerNorm.weight', 'electra.encoder.layer.11.intermediate.dense.bias', 'electra.encoder.layer.0.attention.self.query.weight', 'electra.encoder.layer.8.attention.self.query.weight', 'electra.encoder.layer.3.intermediate.dense.bias', 'electra.encoder.layer.3.attention.output.LayerNorm.weight', 'electra.encoder.layer.9.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.self.key.weight', 'discriminator_predictions.dense_prediction.bias', 'electra.encoder.layer.3.attention.output.dense.bias', 'discriminator_predictions.dense.weight', 'electra.encoder.layer.4.attention.self.value.bias', 'electra.encoder.layer.11.attention.self.query.weight', 'electra.encoder.layer.7.intermediate.dense.bias', 'electra.encoder.layer.8.attention.output.dense.bias', 'electra.encoder.layer.6.attention.output.LayerNorm.bias', 'electra.encoder.layer.3.output.LayerNorm.weight', 'electra.encoder.layer.0.attention.self.value.bias', 'electra.encoder.layer.3.attention.self.value.weight', 'electra.encoder.layer.1.attention.self.query.weight', 'electra.encoder.layer.8.attention.self.key.bias', 'electra.encoder.layer.10.attention.self.key.weight', 'electra.encoder.layer.8.attention.self.key.weight', 'electra.encoder.layer.11.output.dense.bias', 'electra.encoder.layer.1.attention.output.LayerNorm.weight', 'electra.encoder.layer.9.intermediate.dense.bias', 'electra.encoder.layer.4.attention.self.key.weight', 'electra.encoder.layer.9.intermediate.dense.weight', 'discriminator_predictions.dense.bias', 'electra.encoder.layer.9.attention.self.query.weight', 'electra.encoder.layer.5.attention.self.key.weight', 'electra.encoder.layer.7.attention.self.key.bias', 'electra.encoder.layer.7.output.dense.bias', 'electra.encoder.layer.4.attention.self.query.bias', 'electra.encoder.layer.4.intermediate.dense.weight', 'electra.encoder.layer.7.attention.self.query.weight', 'electra.encoder.layer.11.attention.output.LayerNorm.weight', 'electra.embeddings.word_embeddings.weight', 'electra.encoder.layer.4.output.dense.bias', 'electra.encoder.layer.9.attention.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.self.key.weight', 'electra.encoder.layer.0.output.LayerNorm.bias', 'electra.encoder.layer.8.intermediate.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'electra.encoder.layer.5.attention.output.LayerNorm.bias', 'electra.encoder.layer.2.attention.self.value.weight', 'electra.encoder.layer.6.output.dense.weight', 'electra.encoder.layer.1.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.output.LayerNorm.bias', 'electra.encoder.layer.4.attention.output.LayerNorm.bias', 'electra.encoder.layer.4.intermediate.dense.bias', 'electra.encoder.layer.7.output.LayerNorm.bias', 'electra.encoder.layer.10.attention.self.value.weight', 'electra.encoder.layer.2.output.LayerNorm.weight', 'electra.encoder.layer.4.attention.self.key.bias', 'electra.encoder.layer.4.output.dense.weight', 'electra.encoder.layer.6.attention.self.query.weight', 'electra.encoder.layer.6.attention.self.value.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.output.dense.weight', 'electra.encoder.layer.2.attention.self.key.bias', 'electra.encoder.layer.6.attention.output.dense.weight', 'electra.encoder.layer.6.intermediate.dense.bias', 'electra.encoder.layer.8.attention.output.LayerNorm.weight', 'electra.embeddings.position_embeddings.weight', 'electra.encoder.layer.6.attention.output.dense.bias', 'electra.embeddings_project.bias', 'electra.encoder.layer.10.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.self.value.bias', 'electra.encoder.layer.10.output.dense.weight', 'electra.encoder.layer.2.attention.self.query.weight', 'electra.encoder.layer.3.attention.output.dense.weight', 'electra.encoder.layer.10.intermediate.dense.bias', 'electra.encoder.layer.3.attention.self.key.bias', 'electra.encoder.layer.0.attention.output.dense.bias', 'electra.encoder.layer.10.attention.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.self.value.bias', 'electra.encoder.layer.11.attention.self.value.weight', 'electra.encoder.layer.5.attention.self.key.bias', 'electra.encoder.layer.5.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.self.key.weight', 'electra.encoder.layer.1.output.LayerNorm.weight', 'electra.encoder.layer.2.attention.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.self.query.bias', 'electra.encoder.layer.6.attention.self.key.bias', 'electra.embeddings.LayerNorm.weight', 'electra.encoder.layer.7.attention.self.key.weight', 'electra.encoder.layer.0.intermediate.dense.bias', 'electra.encoder.layer.9.attention.self.key.bias', 'electra.encoder.layer.3.output.dense.weight', 'electra.encoder.layer.3.intermediate.dense.weight', 'electra.encoder.layer.5.attention.self.value.weight', 'electra.encoder.layer.7.output.dense.weight', 'electra.encoder.layer.11.output.LayerNorm.weight', 'electra.encoder.layer.3.output.dense.bias', 'electra.encoder.layer.1.output.dense.weight', 'electra.encoder.layer.5.attention.self.query.weight', 'electra.encoder.layer.11.attention.self.key.weight', 'electra.encoder.layer.8.intermediate.dense.bias', 'electra.encoder.layer.0.attention.self.value.weight', 'electra.encoder.layer.8.output.dense.weight', 'electra.encoder.layer.5.intermediate.dense.weight', 'electra.encoder.layer.0.attention.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.output.dense.bias', 'electra.encoder.layer.6.intermediate.dense.weight', 'electra.encoder.layer.4.attention.self.query.weight', 'electra.encoder.layer.2.intermediate.dense.weight', 'electra.encoder.layer.8.attention.self.value.weight', 'electra.encoder.layer.2.attention.self.value.bias', 'electra.encoder.layer.9.attention.output.dense.bias', 'electra.encoder.layer.2.output.LayerNorm.bias', 'electra.encoder.layer.7.attention.output.LayerNorm.bias', 'electra.encoder.layer.9.attention.output.dense.weight', 'electra.embeddings.token_type_embeddings.weight', 'electra.encoder.layer.6.output.LayerNorm.bias', 'electra.encoder.layer.6.attention.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.output.dense.bias', 'electra.encoder.layer.10.attention.self.query.weight', 'electra.encoder.layer.1.intermediate.dense.weight', 'electra.encoder.layer.7.attention.output.dense.weight', 'electra.encoder.layer.7.attention.self.value.weight', 'electra.encoder.layer.10.attention.self.key.bias', 'electra.encoder.layer.3.attention.self.query.weight', 'electra.encoder.layer.2.attention.output.dense.weight', 'electra.encoder.layer.0.attention.self.key.bias', 'electra.encoder.layer.6.output.dense.bias', 'electra.encoder.layer.8.output.dense.bias', 'electra.encoder.layer.0.attention.self.query.bias', 'electra.encoder.layer.10.output.LayerNorm.bias', 'electra.encoder.layer.11.output.LayerNorm.bias', 'electra.encoder.layer.7.intermediate.dense.weight', 'electra.encoder.layer.2.intermediate.dense.bias', 'electra.encoder.layer.11.attention.output.dense.bias', 'electra.encoder.layer.7.attention.self.query.bias', 'electra.encoder.layer.9.attention.self.value.weight', 'electra.encoder.layer.5.attention.output.LayerNorm.weight', 'electra.encoder.layer.10.attention.output.LayerNorm.weight', 'electra.encoder.layer.2.output.dense.bias', 'electra.encoder.layer.5.intermediate.dense.bias', 'electra.encoder.layer.1.output.LayerNorm.bias', 'electra.encoder.layer.10.intermediate.dense.weight', 'electra.encoder.layer.2.output.dense.weight', 'electra.encoder.layer.5.attention.output.dense.bias', 'electra.encoder.layer.11.attention.output.dense.weight', 'electra.encoder.layer.1.attention.output.dense.weight', 'electra.encoder.layer.8.output.LayerNorm.bias', 'electra.encoder.layer.10.output.dense.bias', 'electra.encoder.layer.8.attention.self.query.bias', 'electra.encoder.layer.0.output.LayerNorm.weight', 'electra.encoder.layer.1.attention.self.key.weight', 'electra.encoder.layer.3.attention.output.LayerNorm.bias', 'electra.encoder.layer.5.output.dense.weight', 'electra.encoder.layer.8.output.LayerNorm.weight', 'electra.encoder.layer.6.attention.self.query.bias', 'electra.encoder.layer.9.output.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monsoon-nlp/bangla-electra and are newly initialized: ['encoder.layer.9.output.dense.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.7.output.LayerNorm.bias', 'classifier.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.output.dense.weight', 'classifier.weight', 'encoder.layer.7.attention.self.key.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'pooler.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.self.query.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n",
      "C:\\Users\\meera\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1161, 1.2837424791649843)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ClassificationModel('bert', 'monsoon-nlp/bangla-electra', num_labels=5, use_cuda=True, args={\n",
    "    'reprocess_input_data': True,\n",
    "    'use_cached_eval_features': False,\n",
    "    'overwrite_output_dir': True,\n",
    "    'num_train_epochs': 3,\n",
    "    'silent': True\n",
    "}) # , weight=[2.5, 1.0]\n",
    "model.train_model(train_set.sample(frac=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37c6d118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meera\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong predictions:\n",
      "837 wrong out of 1265\n",
      "accuracy = 33.83399209486166\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{3: 289, 4: 182, 1: 114, 2: 139, 0: 113}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, model_outputs, wrong_predictions = model.eval_model(test_set)\n",
    "bads = {}\n",
    "for pred in wrong_predictions:\n",
    "    if pred.label in bads:\n",
    "        bads[pred.label] += 1\n",
    "    else:\n",
    "        bads[pred.label] = 1\n",
    "print(\"wrong predictions:\")\n",
    "print(str(len(wrong_predictions)) + ' wrong out of ' + str(len(test_set)))\n",
    "print(f\"accuracy = {(len(test_set)-len(wrong_predictions))/len(test_set)*100}\")\n",
    "bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99a4da89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is ***1***\n",
      "test set is ***2***\n"
     ]
    }
   ],
   "source": [
    "train_test_debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16151e",
   "metadata": {},
   "source": [
    "# Report monsoon-nlp/bangla-electra #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058eabf0",
   "metadata": {},
   "source": [
    "train set is ***1***\n",
    "test set is ***1***\n",
    "wrong predictions:\n",
    "474 wrong out of 1032\n",
    "accuracy = 54.06976744186046\n",
    "\n",
    "train set is ***2***\n",
    "test set is ***2***\n",
    "wrong predictions:\n",
    "628 wrong out of 1265\n",
    "accuracy = 50.35573122529644\n",
    "\n",
    "train set is ***1***\n",
    "test set is ***2***\n",
    "wrong predictions:\n",
    "874 wrong out of 1265\n",
    "accuracy = 30.909090909090907\n",
    "\n",
    "train set is ***2***\n",
    "test set is ***1***\n",
    "wrong predictions:\n",
    "699 wrong out of 1032\n",
    "accuracy = 32.26744186046512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866144ba",
   "metadata": {},
   "source": [
    "# Model \"sagorsarker/bangla-bert-base\" #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "874f52ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is ***2***\n",
      "test set is ***1***\n"
     ]
    }
   ],
   "source": [
    "train_test_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbb2d191",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sagorsarker/bangla-bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\meera\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1395, 0.7065620677018251)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert2 = ClassificationModel('bert', 'sagorsarker/bangla-bert-base', num_labels=5, use_cuda=True, args={\n",
    "    'reprocess_input_data': True,\n",
    "    'use_cached_eval_features': False,\n",
    "    'overwrite_output_dir': True,\n",
    "    'num_train_epochs': 3,\n",
    "    'silent': True\n",
    "})\n",
    "bert2.train_model(train_set.sample(frac=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3118ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meera\\anaconda3\\lib\\site-packages\\simpletransformers\\classification\\classification_model.py:1454: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong predictions:\n",
      "578 wrong out of 1032\n",
      "accuracy = 43.992248062015506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: 132, 1: 184, 2: 143, 4: 73, 3: 46}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result, model_outputs, wrong_predictions = bert2.eval_model(test_set)\n",
    "bads = {}\n",
    "for pred in wrong_predictions:\n",
    "    if pred.label in bads:\n",
    "        bads[pred.label] += 1\n",
    "    else:\n",
    "        bads[pred.label] = 1\n",
    "print(\"wrong predictions:\")\n",
    "print(str(len(wrong_predictions)) + ' wrong out of ' + str(len(test_set)))\n",
    "print(f\"accuracy = {(len(test_set)-len(wrong_predictions))/len(test_set)*100}\")\n",
    "bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa0db012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set is ***2***\n",
      "test set is ***1***\n"
     ]
    }
   ],
   "source": [
    "train_test_debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d58ff4",
   "metadata": {},
   "source": [
    "# Report sagorsarker/bangla-bert-base #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39078ab9",
   "metadata": {},
   "source": [
    "train set is ***1***\n",
    "test set is ***1***\n",
    "wrong predictions:\n",
    "214 wrong out of 1032\n",
    "accuracy = 79.26356589147287\n",
    "\n",
    "train set is ***2***\n",
    "test set is ***2***\n",
    "wrong predictions:\n",
    "185 wrong out of 1265\n",
    "accuracy = 85.37549407114624\n",
    "\n",
    "train set is ***1***\n",
    "test set is ***2***\n",
    "wrong predictions:\n",
    "722 wrong out of 1265\n",
    "accuracy = 42.92490118577075\n",
    "\n",
    "train set is ***2***\n",
    "test set is ***1***\n",
    "wrong predictions:\n",
    "578 wrong out of 1032\n",
    "accuracy = 43.992248062015506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1bf4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
