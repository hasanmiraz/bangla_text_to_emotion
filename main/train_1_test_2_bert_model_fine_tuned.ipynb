{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90b005f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276d0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_set = pd.read_csv(\"csv_data/train_set_1.csv\")\n",
    "test_set = pd.read_csv(\"csv_data/train_set_1.csv\")\n",
    "validate_set = pd.read_csv(\"csv_data/test_set_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e27a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2cbc58",
   "metadata": {},
   "source": [
    "# Optoimizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7be37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class makeDataset(Dataset):\n",
    "    def __init__(self, data, max_length=100):\n",
    "        self.data = data\n",
    "        \n",
    "        self.config = {\n",
    "            \"max_length\": max_length,\n",
    "            \"padding\": \"max_length\",\n",
    "            \"return_tensors\": \"pt\",\n",
    "            \"truncation\": True,\n",
    "            \"add_special_tokens\": True\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        value = self.data.iloc[idx]\n",
    "        return value['text'], value['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04b6de",
   "metadata": {},
   "source": [
    "# Training data 2 testing data 2 final testing data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc1a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = makeDataset(train_set)\n",
    "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a587e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = makeDataset(test_set)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d749630",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = makeDataset(validate_set)\n",
    "final_test_dataloader = DataLoader(final_test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2cb64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newEmotionBert(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(newEmotionBert, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.02)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(128, 5)  \n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        # pass the inputs to the model\n",
    "        out = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        x = self.fc1(out[1])\n",
    "        x = self.relu(x)\n",
    "        # output layer\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a2c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"sagorsarker/bangla-bert-base\"\n",
    "bert = BertModel.from_pretrained(bert_model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7764163",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = newEmotionBert(bert)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ed3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252e419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def train(model, dataloader, optimizer, criterion, config):\n",
    "    model.train()  # prep model for training\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        text, labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            text, **config\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # move things to model\n",
    "        logs = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = criterion(logs, labels)\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2028b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, config):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    model.eval()  # prep model for evaluation\n",
    "    for batch in tqdm(dataloader):\n",
    "        text, labels = batch\n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            text, **config\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # move things to model\n",
    "        output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss_p = criterion(output, labels)\n",
    "        # update running validation loss\n",
    "        valid_loss += loss_p.item() * input_ids.size(0)\n",
    "        # calculate accuracy\n",
    "        proba = torch.exp(output)\n",
    "        top_p, top_class = proba.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total, correct, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a68d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "tokenizer_config = {\n",
    "    \"max_length\": 100,\n",
    "    \"padding\": \"max_length\",\n",
    "    \"return_tensors\": \"pt\",\n",
    "    \"truncation\": True,\n",
    "    \"add_special_tokens\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37488d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515be36947ff4a1393d6be4578906b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d77167b89b4e35a0de6489f9e4ff17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:1.391643.. \tValid Loss:1.194696.. \tAccuracy: 55.8463\n",
      "Epoch: 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc0cafdcd4b44ab985b056fe2bf6ba6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f79ba103a34be09a7113050f9665fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:1.176129.. \tValid Loss:1.017836.. \tAccuracy: 62.9522\n",
      "Epoch: 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476c63455ce2420fb0b455f55499ab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f919b3bcd7241cab8b925d43d16ed4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain loss:1.032800.. \tValid Loss:0.969615.. \tAccuracy: 66.1176\n",
      "Epoch: 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805853c1e4ab4daf9f63e283da683207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/194 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14116\\4075389830.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Train Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Now Evaluate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14116\\2322422238.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, criterion, config)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loss_data, valid_loss_data = [], []\n",
    "valid_loss_min = np.Inf\n",
    "since = time.time()\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    e_since = time.time()\n",
    "\n",
    "    # Train Model\n",
    "    train_loss += train(model, train_dataloader, optimizer, criterion, tokenizer_config)\n",
    "\n",
    "    # Now Evaluate\n",
    "    out = evaluate(model, test_dataloader, criterion, tokenizer_config)\n",
    "\n",
    "    total += out[0]\n",
    "    correct += out[1]\n",
    "    valid_loss += out[2]\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    valid_loss = valid_loss / len(test_dataloader.dataset)\n",
    "\n",
    "    # calculate train loss and running loss\n",
    "    train_loss_data.append(train_loss * 100)\n",
    "    valid_loss_data.append(valid_loss * 100)\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"new_emotion_model.pth\")\n",
    "\n",
    "    print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n",
    "          \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n",
    "          \"\\tAccuracy: {:.4f}\".format(correct / total * 100))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c487a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"./new_emotion_model.pth\", map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c15f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in final_test_dataloader:\n",
    "    text, labels = batch\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        text, **tokenizer_config\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # move things to model\n",
    "    output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n",
    "    preds = output.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23304327",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(len(all_labels)):\n",
    "    if all_labels[i]==all_preds[i]:\n",
    "        correct+=1\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"accuracy: {correct/len(all_labels)*100}\")\n",
    "print(f\"total: {total}\")\n",
    "print(f\"correct: {correct}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
