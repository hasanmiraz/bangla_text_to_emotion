{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90b005f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276d0f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_2 = pd.read_csv(\"train_set_2.csv\")\n",
    "test_2 = pd.read_csv(\"train_set_2.csv\")\n",
    "test_1 = pd.read_csv(\"test_set_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12e27a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertModel, BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2cbc58",
   "metadata": {},
   "source": [
    "# Optoimizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc7be37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class makeDataset(Dataset):\n",
    "    def __init__(self, data, max_length=100):\n",
    "        self.data = data\n",
    "        \n",
    "        self.config = {\n",
    "            \"max_length\": max_length,\n",
    "            \"padding\": \"max_length\",\n",
    "            \"return_tensors\": \"pt\",\n",
    "            \"truncation\": True,\n",
    "            \"add_special_tokens\": True\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        value = self.data.iloc[idx]\n",
    "        return value['text'], value['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f04b6de",
   "metadata": {},
   "source": [
    "# Training data 2 testing data 2 final testing data 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc1a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = makeDataset(train_2)\n",
    "train_dataloader = DataLoader(training_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10a587e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = makeDataset(test_2)\n",
    "test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d749630",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_data = makeDataset(test_1)\n",
    "final_test_dataloader = DataLoader(test_data, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2cb64ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class newEmotionBert(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(newEmotionBert, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.02)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768, 128)\n",
    "\n",
    "        # dense layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(128, 5)  \n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask):\n",
    "        # pass the inputs to the model\n",
    "        out = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        x = self.fc1(out[1])\n",
    "        x = self.relu(x)\n",
    "        # output layer\n",
    "        x = self.fc2(self.dropout(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2a2c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = \"sagorsarker/bangla-bert-base\"\n",
    "bert = BertModel.from_pretrained(bert_model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7764163",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = newEmotionBert(bert)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69ed3feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = StepLR(optimizer, step_size=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "252e419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def train(model, dataloader, optimizer, criterion, config):\n",
    "    model.train()  # prep model for training\n",
    "    train_loss = 0\n",
    "    for batch in dataloader:\n",
    "        text, labels = batch\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            text, **config\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # move things to model\n",
    "        logs = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss = criterion(logs, labels)\n",
    "        train_loss += loss.item() * input_ids.size(0)\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2028b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, config):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    valid_loss = 0.0\n",
    "\n",
    "    model.eval()  # prep model for evaluation\n",
    "    for batch in dataloader:\n",
    "        text, labels = batch\n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            text, **config\n",
    "        )\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        token_type_ids = inputs['token_type_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # move things to model\n",
    "        output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        loss_p = criterion(output, labels)\n",
    "        # update running validation loss\n",
    "        valid_loss += loss_p.item() * input_ids.size(0)\n",
    "        # calculate accuracy\n",
    "        proba = torch.exp(output)\n",
    "        top_p, top_class = proba.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total, correct, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a68d722",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "tokenizer_config = {\n",
    "    \"max_length\": 100,\n",
    "    \"padding\": \"max_length\",\n",
    "    \"return_tensors\": \"pt\",\n",
    "    \"truncation\": True,\n",
    "    \"add_special_tokens\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d37488d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "\tTrain loss:1.426847.. \tValid Loss:1.152012.. \tAccuracy: 57.1774\n",
      "Epoch: 2/5\n",
      "\tTrain loss:1.081701.. \tValid Loss:0.859137.. \tAccuracy: 69.1129\n",
      "Epoch: 3/5\n",
      "\tTrain loss:0.875433.. \tValid Loss:0.806463.. \tAccuracy: 70.6720\n",
      "Epoch: 4/5\n",
      "\tTrain loss:0.841356.. \tValid Loss:0.771137.. \tAccuracy: 71.5054\n",
      "Epoch: 5/5\n",
      "\tTrain loss:0.812197.. \tValid Loss:0.768423.. \tAccuracy: 71.6129\n",
      "Training completed in 10m 16s\n"
     ]
    }
   ],
   "source": [
    "train_loss_data, valid_loss_data = [], []\n",
    "valid_loss_min = np.Inf\n",
    "since = time.time()\n",
    "best_loss = np.inf\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    e_since = time.time()\n",
    "\n",
    "    # Train Model\n",
    "    train_loss += train(model, train_dataloader, optimizer, criterion, tokenizer_config)\n",
    "\n",
    "    # Now Evaluate\n",
    "    out = evaluate(model, test_dataloader, criterion, tokenizer_config)\n",
    "\n",
    "    total += out[0]\n",
    "    correct += out[1]\n",
    "    valid_loss += out[2]\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # print training/validation statistics\n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    valid_loss = valid_loss / len(test_dataloader.dataset)\n",
    "\n",
    "    # calculate train loss and running loss\n",
    "    train_loss_data.append(train_loss * 100)\n",
    "    valid_loss_data.append(valid_loss * 100)\n",
    "    \n",
    "    if valid_loss < best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"new_emotion_model.pth\")\n",
    "\n",
    "    print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n",
    "          \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n",
    "          \"\\tAccuracy: {:.4f}\".format(correct / total * 100))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6c487a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./new_emotion_model.pth\", map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c15f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for batch in final_test_dataloader:\n",
    "    text, labels = batch\n",
    "    inputs = tokenizer.batch_encode_plus(\n",
    "        text, **tokenizer_config\n",
    "    )\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    token_type_ids = inputs['token_type_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # move things to model\n",
    "    output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n",
    "    preds = output.detach().cpu().numpy()\n",
    "    preds = np.argmax(preds, axis = 1)\n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23304327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2664\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(all_labels)):\n",
    "    if all_labels[i]==all_preds[i]:\n",
    "        correct+=1\n",
    "print(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eab8f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 71.61290322580646\n",
      "total: 3720\n",
      "correct: 2664\n"
     ]
    }
   ],
   "source": [
    "print(f\"accuracy: {correct/len(all_labels)*100}\")\n",
    "print(f\"total: {total}\")\n",
    "print(f\"correct: {correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359ecba3",
   "metadata": {},
   "source": [
    "# training data1 testing data 1 final testing data 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
